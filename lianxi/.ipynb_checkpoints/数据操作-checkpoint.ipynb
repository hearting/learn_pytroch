{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d53d9828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9184e-39, 9.0000e-39, 1.0561e-38],\n",
      "        [1.0653e-38, 4.1327e-39, 8.9082e-39],\n",
      "        [9.8265e-39, 9.4592e-39, 1.0561e-38],\n",
      "        [1.0653e-38, 1.0469e-38, 9.5510e-39],\n",
      "        [9.0000e-39, 9.0919e-39, 9.2755e-39]])\n",
      "tensor([[0.1347, 0.6764, 0.1301],\n",
      "        [0.5182, 0.5680, 0.0939],\n",
      "        [0.5271, 0.1694, 0.5522],\n",
      "        [0.0924, 0.8929, 0.1222],\n",
      "        [0.1179, 0.2626, 0.5854]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([5.5000, 3.0000])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.4241, -1.2016, -0.8145],\n",
      "        [ 0.1343,  0.7203, -0.0681],\n",
      "        [-1.2904,  2.6236,  1.3711],\n",
      "        [ 0.0289, -0.2356,  0.3686],\n",
      "        [ 0.5205,  0.9050,  1.6398]])\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n",
      "tensor([[ 0.5588, -0.5252, -0.6844],\n",
      "        [ 0.6525,  1.2883,  0.0258],\n",
      "        [-0.7633,  2.7929,  1.9233],\n",
      "        [ 0.1213,  0.6573,  0.4908],\n",
      "        [ 0.6384,  1.1676,  2.2252]])\n",
      "tensor([[ 0.5588, -0.5252, -0.6844],\n",
      "        [ 0.6525,  1.2883,  0.0258],\n",
      "        [-0.7633,  2.7929,  1.9233],\n",
      "        [ 0.1213,  0.6573,  0.4908],\n",
      "        [ 0.6384,  1.1676,  2.2252]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 创建一个5x3的未初始化的Tensor\n",
    "x=torch.empty(5,3)\n",
    "print(x)\n",
    "# 创建一个5x3的随机初始化的Tensor\n",
    "y=torch.rand(5,3)\n",
    "print(y)\n",
    "# 创建一个5x3的long型全0的Tensor\n",
    "z=torch.zeros(5,3,dtype=torch.long)\n",
    "print(z)\n",
    "# 直接根据数据创建:\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(x)\n",
    "x = x.new_ones(5, 3, dtype=torch.float64)  # 返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float) # 指定新的数据类型\n",
    "print(x) \n",
    "print(x.size())\n",
    "print(x.shape)\n",
    "\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "\n",
    "y.add_(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00b1e872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4241, -1.2016, -0.8145])\n",
      "tensor([ 0.4241, -1.2016, -0.8145])\n"
     ]
    }
   ],
   "source": [
    "y = x[0, :]\n",
    "# y += 1\n",
    "print(y)\n",
    "print(x[0, :]) # 源tensor也被改了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab101408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n",
      "tensor([[ 0.4241, -1.2016, -0.8145,  0.1343,  0.7203],\n",
      "        [-0.0681, -1.2904,  2.6236,  1.3711,  0.0289],\n",
      "        [-0.2356,  0.3686,  0.5205,  0.9050,  1.6398]])\n"
     ]
    }
   ],
   "source": [
    "# 用view()来改变Tensor的形状：\n",
    "\n",
    "y = x.view(15)\n",
    "z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来\n",
    "print(x.size(), y.size(), z.size())\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6aa8860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4241, -0.2016,  0.1855],\n",
      "        [ 1.1343,  1.7203,  0.9319],\n",
      "        [-0.2904,  3.6236,  2.3711],\n",
      "        [ 1.0289,  0.7644,  1.3686],\n",
      "        [ 1.5205,  1.9050,  2.6398]], device='cuda:0')\n",
      "tensor([[ 1.4241, -0.2016,  0.1855],\n",
      "        [ 1.1343,  1.7203,  0.9319],\n",
      "        [-0.2904,  3.6236,  2.3711],\n",
      "        [ 1.0289,  0.7644,  1.3686],\n",
      "        [ 1.5205,  1.9050,  2.6398]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca186c0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m next_h \u001b[38;5;241m=\u001b[39m next_h\u001b[38;5;241m.\u001b[39mtanh()\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m next_h\u001b[38;5;241m.\u001b[39msum ()\n\u001b[1;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\pwg\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\pwg\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "x = torch. randn(1, 10)\n",
    "prev_h = torch.randn(1, 20)\n",
    "W_h= torch.randn (20,20)\n",
    "W_x =torch.randn (20, 10)\n",
    "i2h = torch.mm (W_x, x.t())\n",
    "h2h = torch.mm (W_h, prev_h.t())\n",
    "next_h = i2h + h2h \n",
    "next_h = next_h.tanh()\n",
    "loss = next_h.sum ()\n",
    "loss.backward ()\n",
    "print(loss.grad)#求对x的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15423776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
